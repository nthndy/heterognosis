{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f386625-438a-4e1a-b696-d39ce1c852a7",
   "metadata": {},
   "source": [
    "# Zarr Tiling \n",
    "\n",
    "Notebook for taking Opera Phenix tiff output and tiling the images together into zarr mosaics with metadata attribute, specifically for the heterognosis project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adae0225-5c8b-42da-b76a-2af579102fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zarr\n",
    "import dask.array\n",
    "from scipy import stats\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm.auto import tqdm \n",
    "from skimage.io import imread\n",
    "from macrohet import dataio \n",
    "\n",
    "\n",
    "def create_alpha_mask(height, width, feather=0.1):\n",
    "    \"\"\"\n",
    "    Create a 2D alpha mask of shape (height, width).\n",
    "    The mask is 1 in the interior, and over the last `feather` pixels\n",
    "    near each edge, it linearly ramps down to 0.\n",
    "    feather is a fraction of image i.e. a float percentage.\n",
    "    This is the core function that enables smooth image tiling \n",
    "    without dramatic cropping of cell morphology.\n",
    "    \"\"\"\n",
    "    # Coordinates\n",
    "    y = np.arange(height)\n",
    "    x = np.arange(width)\n",
    "\n",
    "    feather_pixels = feather * min(height, width)\n",
    "    \n",
    "    # Distance from left and right edges\n",
    "    dist_x_left  = x\n",
    "    dist_x_right = (width - 1) - x\n",
    "    # Distance from top and bottom edges\n",
    "    dist_y_top    = y\n",
    "    dist_y_bottom = (height - 1) - y\n",
    "    \n",
    "    # For each row, figure out the \"vertical\" ramp factor from top/bottom\n",
    "    # We'll make an array that is the min distance to top or bottom.\n",
    "    ramp_y = np.minimum(dist_y_top, dist_y_bottom)\n",
    "    # For each column, min distance to left or right\n",
    "    ramp_x = np.minimum(dist_x_left, dist_x_right)\n",
    "    \n",
    "    # Combine to get a 2D \"distance from any edge\"\n",
    "    # We can broadcast ramp_y over columns, ramp_x over rows\n",
    "    dist_from_edge = np.minimum(\n",
    "        ramp_y[:, None],\n",
    "        ramp_x[None, :]\n",
    "    )\n",
    "    \n",
    "    # Turn that distance into an alpha that goes from 0 to 1 linearly\n",
    "    # from the very edge to `feather` pixels in.\n",
    "    # If dist_from_edge >= feather, alpha=1. If dist < 0, alpha=0, etc.\n",
    "    alpha = np.clip(dist_from_edge / feather_pixels, 0, 1)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "\n",
    "def load_assay_layout(xml_file_name):\n",
    "    \"\"\"\n",
    "    Loads and parses an assay layout XML file into a pandas DataFrame with a 'position' index.\n",
    "\n",
    "    Args:\n",
    "        xml_file_name (str): The full path to the assay layout XML file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the assay layout information,\n",
    "                          with a 'position' column as index and each layer as a column.\n",
    "                          Returns an empty DataFrame if the file is not found or parsing fails.\n",
    "    \"\"\"\n",
    "    df_assay_layout = pd.DataFrame()\n",
    "\n",
    "    if not os.path.exists(xml_file_name):\n",
    "        print(f\"Error: The file '{xml_file_name}' was not found.\")\n",
    "    else:\n",
    "        try:\n",
    "            tree = ET.parse(xml_file_name)\n",
    "            root = tree.getroot()\n",
    "            namespaces = {'ns': 'http://www.perkinelmer.com/PEHH/HarmonyV5'}\n",
    "            plate_data = {}\n",
    "\n",
    "            for layer in root.findall('ns:Layer', namespaces):\n",
    "                layer_name_element = layer.find('ns:Name', namespaces)\n",
    "                if layer_name_element is None or layer_name_element.text is None:\n",
    "                    print(\"Warning: Found a layer without a name. Skipping this layer.\")\n",
    "                    continue\n",
    "                layer_name = layer_name_element.text.strip()\n",
    "\n",
    "                for well in layer.findall('ns:Well', namespaces):\n",
    "                    row_element = well.find('ns:Row', namespaces)\n",
    "                    col_element = well.find('ns:Col', namespaces)\n",
    "                    value_element = well.find('ns:Value', namespaces)\n",
    "\n",
    "                    if row_element is None or row_element.text is None or \\\n",
    "                       col_element is None or col_element.text is None:\n",
    "                        print(f\"Warning: Found a well in layer '{layer_name}' without row or col. Skipping this well.\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        row = int(row_element.text.strip())\n",
    "                        col = int(col_element.text.strip())\n",
    "                    except ValueError:\n",
    "                        print(f\"Warning: Could not parse row/col as integers for a well in layer '{layer_name}'. Skipping this well.\")\n",
    "                        continue\n",
    "\n",
    "                    value = value_element.text.strip() if value_element is not None and value_element.text is not None else pd.NA\n",
    "                    well_key = (row, col)\n",
    "                    if well_key not in plate_data:\n",
    "                        plate_data[well_key] = {}\n",
    "                    plate_data[well_key][layer_name] = value\n",
    "\n",
    "            if plate_data:\n",
    "                df_assay_layout = pd.DataFrame.from_dict(plate_data, orient='index')\n",
    "                df_assay_layout.index.names = ['Row', 'Col']\n",
    "                df_assay_layout = df_assay_layout.sort_index()\n",
    "                df_assay_layout = df_assay_layout.dropna(how='all')\n",
    "\n",
    "                # Create a new 'position' column\n",
    "                df_assay_layout['position'] = df_assay_layout.index.map(lambda rc: f\"({rc[0]}, {rc[1]})\")\n",
    "                # Set 'position' as the new index\n",
    "                df_assay_layout = df_assay_layout.set_index('position')\n",
    "                df_assay_layout = df_assay_layout.drop(columns=['Row', 'Col']) # Drop the old index columns\n",
    "\n",
    "            else:\n",
    "                print(\"No data was extracted from the XML file to populate the DataFrame.\")\n",
    "\n",
    "        except ET.ParseError as e:\n",
    "            print(f\"Error parsing XML file '{xml_file_name}': {e}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file '{xml_file_name}' was not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during XML processing: {e}\")\n",
    "\n",
    "    return df_assay_layout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238f01f-4ae2-48df-ac49-fa69a691b9c7",
   "metadata": {},
   "source": [
    "### Add acquisition subdirectories\n",
    "\n",
    "This is where all the imagery will be kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93e011-c91e-4854-9031-2de325ce2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/mnt/DATA3/BPP0050/'\n",
    "parent_directories = glob.glob(os.path.join(base_dir, '*/'))\n",
    "\n",
    "# Define the name of the new subdirectory for acquisitions\n",
    "ACQUISITIONS_DIR_NAME = \"acquisition\"\n",
    "\n",
    "print(f\"Starting to organize subdirectories into '{ACQUISITIONS_DIR_NAME}'...\")\n",
    "\n",
    "for parent_dir in parent_directories:\n",
    "    # Ensure the parent directory path is clean and exists\n",
    "    parent_dir = os.path.normpath(parent_dir)\n",
    "    if not os.path.isdir(parent_dir):\n",
    "        print(f\"Warning: Parent directory not found, skipping: {parent_dir}\")\n",
    "        continue\n",
    "\n",
    "    # Construct the path for the new 'acquisitions' subdirectory\n",
    "    acquisitions_path = os.path.join(parent_dir, ACQUISITIONS_DIR_NAME)\n",
    "\n",
    "    # Create the 'acquisitions' subdirectory if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(acquisitions_path, exist_ok=True)\n",
    "        print(f\"Ensured '{acquisitions_path}' exists.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating '{acquisitions_path}': {e}\")\n",
    "        continue # Skip to the next parent directory if creation fails\n",
    "\n",
    "    # List all entries in the parent directory\n",
    "    # Filter for actual directories, excluding the 'acquisitions' directory itself\n",
    "    items_in_parent_dir = [\n",
    "        d for d in os.listdir(parent_dir)\n",
    "        if os.path.isdir(os.path.join(parent_dir, d)) and d != ACQUISITIONS_DIR_NAME\n",
    "    ]\n",
    "\n",
    "    if not items_in_parent_dir:\n",
    "        print(f\"No subdirectories found to move in: {parent_dir}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing parent directory: {parent_dir}\")\n",
    "    for subdir_name in items_in_parent_dir:\n",
    "        source_path = os.path.join(parent_dir, subdir_name)\n",
    "        destination_path = os.path.join(acquisitions_path, subdir_name)\n",
    "\n",
    "        try:\n",
    "            # Move the subdirectory\n",
    "            shutil.move(source_path, destination_path)\n",
    "            print(f\"  Moved '{source_path}' to '{destination_path}'\")\n",
    "        except shutil.Error as e:\n",
    "            print(f\"  Error moving '{source_path}': {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  An unexpected error occurred while moving '{source_path}': {e}\")\n",
    "\n",
    "print(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18bcaa1-7b7b-4632-b4f7-09726eed5786",
   "metadata": {},
   "source": [
    "# Tile, add metadata and save out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b99c47cd-497e-4732-9588-6dcc68d0ee16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b013a2204040069a18be4feab6db7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading metadata XML file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f7bf388a26407e95cc8bb7dcda05f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:12: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  metadata = metadata.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred during XML processing: \"['Row', 'Col'] not found in axis\"\n",
      "Now tile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37d32cab2214dc0b7e25c0e2e08e5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(2, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(2, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(2, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(2, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(2, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(3, 2).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(3, 3).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(3, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(3, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(3, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(3, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(3, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(3, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(4, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(4, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(4, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(4, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(4, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(4, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(5, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr/(5, 5).zarr already exists, skipping\n",
      "Warning: Less than two unique TimepointIDs found, skipping framerate calculation.\n",
      "OME-Zarr metadata and assay layout saved to: /mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy5__2025-04-18T16_38_51-Measurement 1/acquisition/zarr//.zattrs\n",
      "Reading metadata XML file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:152: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  timepoint_abs_times = metadata[['TimepointID', 'AbsTime']].drop_duplicates(subset=['TimepointID']).sort_values(key=lambda x: pd.to_numeric(x, errors='ignore'), by='TimepointID')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4fcd9d31a7443e8beb37397fbca817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:12: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  metadata = metadata.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred during XML processing: \"['Row', 'Col'] not found in axis\"\n",
      "Now tile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f438c966748c491fb8ccda7b8f076eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(2, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(2, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(2, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(2, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(2, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(3, 2).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(3, 3).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(3, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(3, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(3, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(3, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(3, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(3, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(4, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(4, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(4, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(4, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(4, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(4, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(5, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr/(5, 5).zarr already exists, skipping\n",
      "Warning: Less than two unique TimepointIDs found, skipping framerate calculation.\n",
      "OME-Zarr metadata and assay layout saved to: /mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy8__2025-04-21T16_49_44-Measurement 1/acquisition/zarr//.zattrs\n",
      "Reading metadata XML file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:152: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  timepoint_abs_times = metadata[['TimepointID', 'AbsTime']].drop_duplicates(subset=['TimepointID']).sort_values(key=lambda x: pd.to_numeric(x, errors='ignore'), by='TimepointID')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96f9648f3c84e04831c8706b8197b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:12: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  metadata = metadata.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred during XML processing: \"['Row', 'Col'] not found in axis\"\n",
      "Now tile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649933c6e3f444abaeab8d8faee7c649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(2, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(2, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(2, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(2, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(2, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(3, 2).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(3, 3).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(3, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(3, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(3, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(3, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(3, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(3, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(4, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(4, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(4, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(4, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(4, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(4, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(5, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr/(5, 5).zarr already exists, skipping\n",
      "Warning: Less than two unique TimepointIDs found, skipping framerate calculation.\n",
      "OME-Zarr metadata and assay layout saved to: /mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy10__2025-04-23T15_45_30-Measurement 1/acquisition/zarr//.zattrs\n",
      "Reading metadata XML file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:152: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  timepoint_abs_times = metadata[['TimepointID', 'AbsTime']].drop_duplicates(subset=['TimepointID']).sort_values(key=lambda x: pd.to_numeric(x, errors='ignore'), by='TimepointID')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688cf66876b245c682958d5a8e545b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:12: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  metadata = metadata.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred during XML processing: \"['Row', 'Col'] not found in axis\"\n",
      "Now tile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ced74c79ee24a139def22da308f2843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(2, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(2, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(2, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(2, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(2, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(3, 2).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(3, 3).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(3, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(3, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(3, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(3, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(3, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(3, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(4, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(4, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(4, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(4, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(4, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(4, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(5, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr/(5, 5).zarr already exists, skipping\n",
      "Warning: Less than two unique TimepointIDs found, skipping framerate calculation.\n",
      "OME-Zarr metadata and assay layout saved to: /mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy2__2025-04-15T16_42_33-Measurement 3/acquisition/zarr//.zattrs\n",
      "Reading metadata XML file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:152: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  timepoint_abs_times = metadata[['TimepointID', 'AbsTime']].drop_duplicates(subset=['TimepointID']).sort_values(key=lambda x: pd.to_numeric(x, errors='ignore'), by='TimepointID')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a2cbf680194706a769e492f068d91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:12: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  metadata = metadata.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred during XML processing: \"['Row', 'Col'] not found in axis\"\n",
      "Now tile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706e2af99b7e4eb49a42ff11f485e969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(2, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(2, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(2, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(2, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(2, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(3, 2).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(3, 3).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(3, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(3, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(3, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(3, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(3, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(3, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(4, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(4, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(4, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(4, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(4, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(4, 9).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(5, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr/(5, 5).zarr already exists, skipping\n",
      "Warning: Less than two unique TimepointIDs found, skipping framerate calculation.\n",
      "OME-Zarr metadata and assay layout saved to: /mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Fixed_Cy4__2025-04-17T16_24_52-Measurement 1/acquisition/zarr//.zattrs\n",
      "Reading metadata XML file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:152: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  timepoint_abs_times = metadata[['TimepointID', 'AbsTime']].drop_duplicates(subset=['TimepointID']).sort_values(key=lambda x: pd.to_numeric(x, errors='ignore'), by='TimepointID')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d56b7c08654cb58d386ccd346ea91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:12: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  metadata = metadata.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred during XML processing: \"['Row', 'Col'] not found in axis\"\n",
      "Now tile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba2516b6ebb4fbc8fa55ba0f9373504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(3, 3).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(3, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(3, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(3, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(3, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(3, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(4, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(4, 5).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(4, 6).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(4, 7).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(4, 8).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(5, 4).zarr already exists, skipping\n",
      "/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr/(5, 5).zarr already exists, skipping\n",
      "Calculated framerate: 0.0005555488377486879 frames per second\n",
      "OME-Zarr metadata and assay layout saved to: /mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-1__2025-04-09T18_25_04-Measurement 1/acquisition/zarr//.zattrs\n",
      "Reading metadata XML file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877104/195813770.py:152: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  timepoint_abs_times = metadata[['TimepointID', 'AbsTime']].drop_duplicates(subset=['TimepointID']).sort_values(key=lambda x: pd.to_numeric(x, errors='ignore'), by='TimepointID')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-2__2025-04-10T18_45_48-Measurement 1/acquisition/Images/Index.idx.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m base_dir_ID \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(base_dir)\n\u001b[1;32m      8\u001b[0m metadata_fn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macquisition/Images/Index.idx.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43mdataio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_harmony_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_fn\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Convert applicable columns to numeric values\u001b[39;00m\n\u001b[1;32m     12\u001b[0m metadata \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mto_numeric, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/analysis/macrodev/macrohet/dataio.py:229\u001b[0m, in \u001b[0;36mread_harmony_metadata\u001b[0;34m(metadata_path, assay_layout, mask_exist, image_dir, image_metadata, replicate_number, iter)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Handle the iteration mode with iterparse (iter=True)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m assay_layout \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28miter\u001b[39m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Use the iterative parsing method\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event, elem \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mET_iter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# Check for the 'Images' tag in the element\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mtag:\n\u001b[1;32m    232\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m image_metadata \u001b[38;5;129;01min\u001b[39;00m elem:\n",
      "File \u001b[0;32msrc/lxml/iterparse.pxi:79\u001b[0m, in \u001b[0;36mlxml.etree.iterparse.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/DATA3/BPP0050/BPP0050-1-Live-cell-to4i_Live-2__2025-04-10T18_45_48-Measurement 1/acquisition/Images/Index.idx.xml'"
     ]
    }
   ],
   "source": [
    "base_dir = '/mnt/DATA3/BPP0050/'\n",
    "base_dirs = glob.glob(os.path.join(base_dir, '*'))\n",
    "\n",
    "for base_dir in tqdm(base_dirs, total = len(base_dirs)):\n",
    "\n",
    "    base_dir_ID = os.path.basename(base_dir)\n",
    "    \n",
    "    metadata_fn = os.path.join(base_dir, 'acquisition/Images/Index.idx.xml')\n",
    "    metadata = dataio.read_harmony_metadata(metadata_fn)  \n",
    "\n",
    "    # Convert applicable columns to numeric values\n",
    "    metadata = metadata.apply(pd.to_numeric, errors='ignore')\n",
    "    # Invert Y coordinates (cathode ray tube adjustment)\n",
    "    metadata['PositionY'] = -metadata['PositionY']\n",
    "    \n",
    "    # Find the minimum Z position for non-Phase Contrast channels\n",
    "    min_z = metadata.loc[~metadata['ChannelName'].str.contains('Phase Contrast', case=False, na=False), 'PositionZ'].min()\n",
    "    \n",
    "    # Assign this minimum Z value to all \"Phase Contrast\" channels\n",
    "    metadata.loc[metadata['ChannelName'].str.contains('Phase Contrast', case=False, na=False), 'PositionZ'] = min_z\n",
    "    \n",
    "    image_dir = os.path.join(base_dir, 'acquisition/Images/')\n",
    "    # use the metadata for the rows cols as the assay layout can be the same for multiple acquisitions, encompassing multiple layouts\n",
    "    rows_cols = metadata[['Row', 'Col']].drop_duplicates().values\n",
    "    \n",
    "    assay_layout_fn = glob.glob(os.path.join(base_dir, 'acquisition/Assaylayout/*.xml'))[0]\n",
    "    assay_layout = load_assay_layout(assay_layout_fn)\n",
    "\n",
    "\n",
    "\n",
    "    print('Now tile')\n",
    "    for row_col in tqdm(rows_cols, total = len(rows_cols)):\n",
    "        row, column = int(row_col[0]), int(row_col[1])\n",
    "        col = column\n",
    "        acq_ID = row, column\n",
    "        \n",
    "        if os.path.exists(os.path.join(base_dir, f'acquisition/zarr/{acq_ID}.zarr')):\n",
    "            print(os.path.join(base_dir, f'acquisition/zarr/{acq_ID}.zarr'), 'already exists, skipping')\n",
    "            continue\n",
    "\n",
    "        # Filter metadata for the current row and column (include all time points)\n",
    "        position_metadata = metadata[(metadata['Row'] == row) & (metadata['Col'] == column)]\n",
    "        # Get unique time points\n",
    "        timepoint_ids = sorted(position_metadata['TimepointID'].unique())\n",
    "        # Initialize a list to store time-lapse volumes\n",
    "        time_volumes = []\n",
    "        \n",
    "        sample_tile_id =  position_metadata.iloc[0]\n",
    "        # Iterate over each time point\n",
    "        for timepoint_id in tqdm(timepoint_ids, desc=\"Processing Time Points\", leave=True, position=1):\n",
    "            # Filter metadata for the current time point\n",
    "            time_metadata = position_metadata[position_metadata['TimepointID'] == timepoint_id]\n",
    "            # Get unique Channel\n",
    "            channel_ids = sorted(time_metadata['ChannelID'].unique())\n",
    "            # Initialize a list to hold each channel's Z-stack for this time point\n",
    "            channel_volumes = []\n",
    "            # Iterate over each channel\n",
    "            for channel_id in tqdm(channel_ids, desc=f\"Processing Channels (T={timepoint_id})\", leave=False, position=2):\n",
    "                # get specific channel metadata\n",
    "                channel_slice_metadata = time_metadata[time_metadata['ChannelID'] == channel_id]\n",
    "                # get z positions for this channel\n",
    "                z_ids = sorted(channel_slice_metadata['PositionZ'].unique())\n",
    "                # init a list to hold z slices\n",
    "                z_slices = []\n",
    "                # Iterate over Z positions\n",
    "                for z_id in z_ids:#tqdm(z_positions, desc=\"Processing Z-slices\", leave=False, position=3):\n",
    "                    # get specific z slice metadata\n",
    "                    z_slice_metadata = channel_slice_metadata[channel_slice_metadata['PositionZ'] == z_id]\n",
    "                    # Initialize mosaic_slice if needed, ie for the first image to be placed in the blank slate\n",
    "                    mosaic_size_x = int((z_slice_metadata['PositionX'].max() - z_slice_metadata['PositionX'].min()) / sample_tile_id['ImageResolutionX']) + sample_tile_id['ImageSizeX']\n",
    "                    mosaic_size_y = int((z_slice_metadata['PositionY'].max() - z_slice_metadata['PositionY'].min()) / sample_tile_id['ImageResolutionY']) + sample_tile_id['ImageSizeY']\n",
    "                    # For each position, create a large mosaic (accumulator & weight)\n",
    "                    # Suppose mosaic_size_y, mosaic_size_x is the total bounding box in Y and X.\n",
    "                    accumulator = np.zeros((mosaic_size_y, mosaic_size_x), dtype=np.float32)\n",
    "                    weight_map  = np.zeros((mosaic_size_y, mosaic_size_x), dtype=np.float32)\n",
    "                    \n",
    "                    for tile_index, tile_id in z_slice_metadata.iterrows():\n",
    "                        # load the tile\n",
    "                        img_path = os.path.join(image_dir, tile_id['URL'])\n",
    "                        try:\n",
    "                            tile = imread(img_path).astype(np.float32)\n",
    "                        except FileNotFoundError:\n",
    "                            tile = np.zeros((tile_id['ImageSizeY'], tile_id['ImageSizeX']), dtype=np.float32)\n",
    "                    \n",
    "                        # create alpha mask for this tile's shape\n",
    "                        alpha_mask = create_alpha_mask(tile.shape[0], tile.shape[1], feather=0.1)\n",
    "                    \n",
    "                        # figure out where to place this tile in the mosaic\n",
    "                        x_pixel = int((tile_id['PositionX'] - z_slice_metadata['PositionX'].min()) / tile_id['ImageResolutionX'])\n",
    "                        y_pixel = int((tile_id['PositionY'] - z_slice_metadata['PositionY'].min()) / tile_id['ImageResolutionY'])\n",
    "                    \n",
    "                        # add into the accumulator & weight\n",
    "                        # We slice out the region in the mosaic that corresponds to this tile\n",
    "                        acc_slice = accumulator[y_pixel:y_pixel+tile.shape[0], x_pixel:x_pixel+tile.shape[1]]\n",
    "                        wgt_slice = weight_map[y_pixel:y_pixel+tile.shape[0], x_pixel:x_pixel+tile.shape[1]]\n",
    "                    \n",
    "                        acc_slice += tile * alpha_mask\n",
    "                        wgt_slice += alpha_mask\n",
    "                    \n",
    "                    # AFTER all tiles are placed:\n",
    "                    final_mosaic = np.zeros_like(accumulator, dtype=np.float32)\n",
    "                    \n",
    "                    # Avoid division by zero\n",
    "                    nonzero_mask = (weight_map > 0)\n",
    "                    final_mosaic[nonzero_mask] = accumulator[nonzero_mask] / weight_map[nonzero_mask]\n",
    "                    \n",
    "                    # Then, convert final mosaic to e.g. uint16 if desired\n",
    "                    mosaic = np.clip(final_mosaic, 0, 65535).astype(np.uint16)\n",
    "                    \n",
    "                    # now append that mosaic z slice to z slices stack, different if DPC\n",
    "                    if \"Phase Contrast\" in channel_slice_metadata['ChannelName'].iloc[0]:  \n",
    "                        # print(f\"Distributing DPC image across Z slices\")\n",
    "                        dpc_slice = mosaic / len(metadata['PositionZ'].unique())  \n",
    "                        z_slices = [mosaic] * len(metadata['PositionZ'].unique())  \n",
    "                        break  \n",
    "                    else:\n",
    "                        z_slices.append(mosaic)\n",
    "                    \n",
    "                #stack z slices into a channel volume\n",
    "                if len(z_slices) > 0:\n",
    "                    # channel_volume = np.max(z_slices, axis=0)\n",
    "                    channel_volume = np.stack(z_slices, axis=0)\n",
    "                    channel_volumes.append(channel_volume)\n",
    "            # stack the channel volume into a time volume\n",
    "            if len(channel_volumes) > 0:\n",
    "                image_volume_czyx = np.stack(channel_volumes, axis=0)  # C, Z, Y, X\n",
    "                time_volumes.append(image_volume_czyx)  # Store for this time point\n",
    "        # Stack all time points into a 5D array (T, C, Z, Y, X)\n",
    "        if len(time_volumes) > 0:\n",
    "            images = np.stack(time_volumes, axis=0)  # T, C, Z, Y, X\n",
    "            print(f\"Final image volume shape: {images.shape}\")\n",
    "\n",
    "        try:\n",
    "            zarr_output_dir = os.path.join(base_dir, f'acquisition/zarr/{acq_ID}.zarr')\n",
    "            dask_images = dask.array.from_array(images)\n",
    "            zarr_group = zarr.open(zarr_output_dir, mode='w')\n",
    "            dask.array.to_zarr(dask_images, zarr_output_dir, component='images')\n",
    "            print(f\"Saved Zarr array successfully to {zarr_output_dir}\")\n",
    "        except:\n",
    "            print(base_dir_ID, acq_ID, 'zarr output failed')\n",
    "\n",
    "    # --- Omero Metadata ---\n",
    "    omero_metadata = {}\n",
    "    average_time_difference_seconds = None\n",
    "\n",
    "    # Calculate temporal framerate based on unique, sorted TimepointIDs\n",
    "    if 'AbsTime' in metadata.columns and 'TimepointID' in metadata.columns:\n",
    "        try:\n",
    "            if not pd.api.types.is_datetime64_any_dtype(metadata['AbsTime']):\n",
    "                metadata['AbsTime'] = pd.to_datetime(metadata['AbsTime'], format='ISO8601', utc=True)\n",
    "\n",
    "            timepoint_abs_times = metadata[['TimepointID', 'AbsTime']].drop_duplicates(subset=['TimepointID']).sort_values(key=lambda x: pd.to_numeric(x, errors='ignore'), by='TimepointID')\n",
    "\n",
    "            if len(timepoint_abs_times['TimepointID'].unique()) > 1: # Check if there is a time series\n",
    "                # Calculate the time difference between consecutive timepoints in seconds\n",
    "                time_diffs = timepoint_abs_times['AbsTime'].diff().dt.total_seconds().dropna()\n",
    "\n",
    "                if not time_diffs.empty:\n",
    "                    # Calculate the average time difference in seconds\n",
    "                    average_time_difference_seconds = time_diffs.mean()\n",
    "                    if average_time_difference_seconds > 0:\n",
    "                        framerate = 1 / average_time_difference_seconds\n",
    "                        omero_metadata['frameRate'] = framerate\n",
    "                        print(f\"Calculated framerate: {framerate} frames per second\")\n",
    "                    else:\n",
    "                        print(\"Warning: Average time difference is zero or negative, cannot calculate framerate.\")\n",
    "                else:\n",
    "                    print(\"Warning: Could not calculate time differences for framerate.\")\n",
    "            else:\n",
    "                print(\"Warning: Less than two unique TimepointIDs found, skipping framerate calculation.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: An error occurred while calculating framerate: {e}\")\n",
    "    # Channels (rest of your channel metadata code remains the same)\n",
    "    channels_data = []\n",
    "    for index, row in metadata.drop_duplicates(subset=['ChannelID']).iterrows():\n",
    "        emission_wavelength_meters = None\n",
    "        excitation_wavelength_meters = None\n",
    "    \n",
    "        if pd.notna(row['MainEmissionWavelength']):\n",
    "            try:\n",
    "                emission_wavelength = float(row['MainEmissionWavelength'])\n",
    "                emission_wavelength_meters = emission_wavelength * 1e-9\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not convert '{row['MainEmissionWavelength']}' to float for emission wavelength (Channel ID: {row['ChannelID']}). Setting to None.\")\n",
    "    \n",
    "        if pd.notna(row['MainExcitationWavelength']):\n",
    "            try:\n",
    "                excitation_wavelength = float(row['MainExcitationWavelength'])\n",
    "                excitation_wavelength_meters = excitation_wavelength * 1e-9\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not convert '{row['MainExcitationWavelength']}' to float for excitation wavelength (Channel ID: {row['ChannelID']}). Setting to None.\")\n",
    "    \n",
    "        channel = {\n",
    "            \"id\": int(row['ChannelID']),\n",
    "            \"label\": row['ChannelName'],\n",
    "            \"emissionWaveMeters\": emission_wavelength_meters,\n",
    "            \"excitationWaveMeters\": excitation_wavelength_meters,\n",
    "            # Add other relevant channel metadata if available\n",
    "        }\n",
    "        channels_data.append(channel)\n",
    "    omero_metadata['channels'] = channels_data\n",
    "    \n",
    "    # Objective (rest of your objective metadata code remains the same)\n",
    "    objective_data = {}\n",
    "    if 'ObjectiveMagnification' in metadata.columns and pd.notna(metadata['ObjectiveMagnification'].iloc[0]):\n",
    "        try:\n",
    "            objective_data['magnification'] = float(metadata['ObjectiveMagnification'].iloc[0])\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Could not convert '{metadata['ObjectiveMagnification'].iloc[0]}' to float for objective magnification. Skipping.\")\n",
    "    if 'ObjectiveNA' in metadata.columns and pd.notna(metadata['ObjectiveNA'].iloc[0]):\n",
    "        try:\n",
    "            objective_data['numericalAperture'] = float(metadata['ObjectiveNA'].iloc[0])\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Could not convert '{metadata['ObjectiveNA'].iloc[0]}' to float for objective NA. Skipping.\")\n",
    "    if objective_data:\n",
    "        omero_metadata['objective'] = objective_data\n",
    "\n",
    "    # --- Multiscales Metadata ---\n",
    "    multiscales_data = [{\n",
    "        \"name\": \"images\",\n",
    "        \"datasets\": [],\n",
    "        \"axes\": [\n",
    "            {\"name\": \"t\", \"type\": \"time\", \"unit\": \"second\", \"spacing\": average_time_difference_seconds},\n",
    "            {\"name\": \"c\", \"type\": \"channel\"},\n",
    "            {\"name\": \"z\", \"type\": \"space\", \"unit\": \"meter\", \"spacing\": None},\n",
    "            {\"name\": \"y\", \"type\": \"space\", \"unit\": \"meter\", \"pixelResolution\": None},\n",
    "            {\"name\": \"x\", \"type\": \"space\", \"unit\": \"meter\", \"pixelResolution\": None}\n",
    "        ],\n",
    "        \"type\": \"image\",\n",
    "        \"version\": \"0.4\"\n",
    "    }]\n",
    "\n",
    "    # Iterate through your existing Zarr directories to populate datasets (remains the same)\n",
    "    zarr_root_dir = f'{base_dir}/acquisition/zarr/'\n",
    "    for filename in os.listdir(zarr_root_dir):\n",
    "        if filename.endswith('.zarr'):\n",
    "            position_name = filename.replace('.zarr', '')\n",
    "            multiscales_data[0]['datasets'].append({\"path\": f\"{filename}/images\"})\n",
    "\n",
    "    # Add Image resolution and size information (remains the same)\n",
    "    if 'ImageResolutionX' in metadata.columns and 'ImageResolutionY' in metadata.columns:\n",
    "        try:\n",
    "            resolution_x = float(metadata['ImageResolutionX'].iloc[0])\n",
    "            resolution_y = float(metadata['ImageResolutionY'].iloc[0])\n",
    "            multiscales_data[0]['axes'][3]['pixelResolution'] = resolution_y\n",
    "            multiscales_data[0]['axes'][4]['pixelResolution'] = resolution_x\n",
    "        except ValueError:\n",
    "            print(\"Warning: Could not convert ImageResolutionX or ImageResolutionY to float. Skipping resolution metadata.\")\n",
    "        except IndexError:\n",
    "            print(\"Warning: Could not set pixelResolution, check axes configuration.\")\n",
    "\n",
    "    if 'ImageSizeX' in metadata.columns and 'ImageSizeY' in metadata.columns:\n",
    "        try:\n",
    "            size_x = int(metadata['ImageSizeX'].iloc[0])\n",
    "            size_y = int(metadata['ImageSizeY'].iloc[0])\n",
    "        except ValueError:\n",
    "            print(\"Warning: Could not convert ImageSizeX or ImageSizeY to int. Skipping image size metadata.\")\n",
    "\n",
    "    # Calculate mode of differences between unique Z positions for pseudo z-resolution (remains the same)\n",
    "    if 'PositionZ' in metadata.columns:\n",
    "        try:\n",
    "            z_positions = metadata['PositionZ'].astype(float).unique()\n",
    "            z_positions_sorted = np.sort(z_positions)\n",
    "            if len(z_positions_sorted) > 1:\n",
    "                z_diffs = np.diff(z_positions_sorted)\n",
    "                mode_result = stats.mode(z_diffs)\n",
    "                if mode_result.count > 0:\n",
    "                    modal_z_diff = mode_result.mode\n",
    "                    multiscales_data[0]['axes'][2]['spacing'] = modal_z_diff # in meters\n",
    "                    multiscales_data[0]['axes'][2]['unit'] = \"meter\"\n",
    "                else:\n",
    "                    print(\"Warning: No unique mode found for z-position differences.\")\n",
    "            else:\n",
    "                print(\"Warning: Only one unique z position found, cannot calculate z difference mode.\")\n",
    "        except ValueError:\n",
    "            print(\"Warning: Could not process PositionZ values for z-resolution calculation.\")\n",
    "        except IndexError:\n",
    "            print(\"Warning: Could not set z spacing, check axes configuration.\")\n",
    "\n",
    "\n",
    "    # --- Combine Metadata and save ---\n",
    "    top_level_attrs = {\n",
    "        \"assay_layout\": assay_layout.to_dict(orient='index'),\n",
    "        \"omero\": omero_metadata,\n",
    "        \"multiscales\": multiscales_data,\n",
    "        \"version\": \"0.4\",\n",
    "    }\n",
    "\n",
    "    # Create the top-level Zarr group and save metadata\n",
    "    group = zarr.group(zarr_root_dir, overwrite=False)\n",
    "    group.attrs.put(top_level_attrs)\n",
    "\n",
    "    print(f\"OME-Zarr metadata and assay layout saved to: {zarr_root_dir}/.zattrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9bbc47d-07fc-4762-bbc6-16bc2c0bc28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assay_layout': {(2, 4): {'Infection status': 'None',\n",
       "   'Antibiotic treatment': 'None'},\n",
       "  (2, 5): {'Infection status': 'None', 'Antibiotic treatment': 'PZA EC99'},\n",
       "  (2, 6): {'Infection status': 'None', 'Antibiotic treatment': 'RIF EC99'},\n",
       "  (2, 7): {'Infection status': 'None', 'Antibiotic treatment': 'INH EC99'},\n",
       "  (2, 8): {'Infection status': 'None', 'Antibiotic treatment': 'BDQ EC99'},\n",
       "  (3, 2): {'Infection status': 'None', 'Antibiotic treatment': 'None'},\n",
       "  (3, 3): {'Infection status': 'None', 'Antibiotic treatment': 'None'},\n",
       "  (3, 4): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'None'},\n",
       "  (3, 5): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'PZA EC99'},\n",
       "  (3, 6): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'RIF EC99'},\n",
       "  (3, 7): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'INH EC99'},\n",
       "  (3, 8): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'BDQ EC99'},\n",
       "  (3, 9): {'Infection status': 'CT_odd cycle', 'Antibiotic treatment': None},\n",
       "  (4, 4): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'None'},\n",
       "  (4, 5): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'PZA EC99'},\n",
       "  (4, 6): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'RIF EC99'},\n",
       "  (4, 7): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'INH EC99'},\n",
       "  (4, 8): {'Infection status': 'MtbWT', 'Antibiotic treatment': 'BDQ EC99'},\n",
       "  (4, 9): {'Infection status': 'CT_even cycle', 'Antibiotic treatment': None},\n",
       "  (5, 4): {'Infection status': 'MtbRD1', 'Antibiotic treatment': 'None'},\n",
       "  (5, 5): {'Infection status': 'MtbRD1', 'Antibiotic treatment': 'None'}},\n",
       " 'omero': {'channels': [{'id': 1,\n",
       "    'label': 'EGFP',\n",
       "    'emissionWaveMeters': 5.22e-07,\n",
       "    'excitationWaveMeters': 4.88e-07},\n",
       "   {'id': 2,\n",
       "    'label': 'Alexa 647',\n",
       "    'emissionWaveMeters': 7.06e-07,\n",
       "    'excitationWaveMeters': 6.4e-07},\n",
       "   {'id': 3,\n",
       "    'label': 'Alexa 568',\n",
       "    'emissionWaveMeters': 5.99e-07,\n",
       "    'excitationWaveMeters': 5.61e-07},\n",
       "   {'id': 4,\n",
       "    'label': 'DAPI',\n",
       "    'emissionWaveMeters': 4.56e-07,\n",
       "    'excitationWaveMeters': 4.0500000000000004e-07}],\n",
       "  'objective': {'magnification': 63.0, 'numericalAperture': 1.15}},\n",
       " 'multiscales': [{'name': 'images',\n",
       "   'datasets': [{'path': '(2, 4).zarr/images'},\n",
       "    {'path': '(3, 5).zarr/images'},\n",
       "    {'path': '(3, 7).zarr/images'},\n",
       "    {'path': '(2, 5).zarr/images'},\n",
       "    {'path': '(4, 7).zarr/images'},\n",
       "    {'path': '(3, 6).zarr/images'},\n",
       "    {'path': '(3, 2).zarr/images'},\n",
       "    {'path': '(3, 4).zarr/images'},\n",
       "    {'path': '(2, 7).zarr/images'},\n",
       "    {'path': '(3, 3).zarr/images'},\n",
       "    {'path': '(2, 6).zarr/images'},\n",
       "    {'path': '(3, 9).zarr/images'},\n",
       "    {'path': '(4, 5).zarr/images'},\n",
       "    {'path': '(4, 6).zarr/images'},\n",
       "    {'path': '(4, 8).zarr/images'},\n",
       "    {'path': '(4, 4).zarr/images'},\n",
       "    {'path': '(3, 8).zarr/images'},\n",
       "    {'path': '(4, 9).zarr/images'},\n",
       "    {'path': '(5, 4).zarr/images'},\n",
       "    {'path': '(2, 8).zarr/images'},\n",
       "    {'path': '(5, 5).zarr/images'}],\n",
       "   'axes': [{'name': 't', 'type': 'time', 'unit': 'second', 'spacing': None},\n",
       "    {'name': 'c', 'type': 'channel'},\n",
       "    {'name': 'z', 'type': 'space', 'unit': 'meter', 'spacing': 1e-06},\n",
       "    {'name': 'y',\n",
       "     'type': 'space',\n",
       "     'unit': 'meter',\n",
       "     'pixelResolution': 1.898336764942101e-07},\n",
       "    {'name': 'x',\n",
       "     'type': 'space',\n",
       "     'unit': 'meter',\n",
       "     'pixelResolution': 1.898336764942101e-07}],\n",
       "   'type': 'image',\n",
       "   'version': '0.4'}],\n",
       " 'version': '0.4'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_level_attrs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "godspeed",
   "language": "python",
   "name": "godspeed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
